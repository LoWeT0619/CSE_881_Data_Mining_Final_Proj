{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8875cff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ee88d88",
   "metadata": {},
   "source": [
    "# 1 Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de226b",
   "metadata": {},
   "source": [
    "## 1.1 Import Library and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5801c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99457a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib and load the data\n",
    "\n",
    "import scipy.sparse as sp # we need to go to the gnn environment to install scipy - conda activate gnn, pip3 install scipy\n",
    "import numpy as np\n",
    "import json\n",
    "adj = sp.load_npz('/Users/xinyun/Desktop/2023Spring/CSE881/data_2023/adj.npz')\n",
    "feat  = np.load('/Users/xinyun/Desktop/2023Spring/CSE881/data_2023/features.npy')\n",
    "labels = np.load('/Users/xinyun/Desktop/2023Spring/CSE881/data_2023/labels.npy')\n",
    "splits = json.load(open('/Users/xinyun/Desktop/2023Spring/CSE881/data_2023/splits.json'))\n",
    "idx_train, idx_test = splits['idx_train'], splits['idx_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ee160f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_train = feat[splits['idx_train']]\n",
    "feat_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "c242b629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1385"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from numpy.linalg import matrix_rank\n",
    "#matrix_rank(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "2e1b289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A=np.corrcoef(feat, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e3f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0,1390):\n",
    "#    for j in range(0,1390):\n",
    "#        if (i!=j) & (A[i,j]>0.9):\n",
    "#            print(i,j,A[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080d9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! structure\n",
    "# load structure \n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "edge_index = from_scipy_sparse_matrix(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a851159e",
   "metadata": {},
   "source": [
    "## 1.2 Done with Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "447a8ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! structure\n",
    "edge_index #edges with index pair\n",
    "edge_index = edge_index[0]\n",
    "# 1) show what edge_index looks like - a tensor; 2) show the size of the edge index, 2 row, 10100 colums -> 10100 edges \n",
    "# for the whole graph\n",
    "# edge_index,edge_index.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7731d3",
   "metadata": {},
   "source": [
    "## 1.3 Done with Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4860c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat\n",
    "# ! for our project, we only use the feature of the first 496 nodes/rows \n",
    "# ! the reason: the rest is used by TA to test our performance.\n",
    "# feat.shape \n",
    "# shape of feat is 2480 x 1390; there is 2480 nodes; each node has 1390 features. \n",
    "feat_tsr = torch.from_numpy(feat) # data type: 2480 x 1390 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a085b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all features to float type\n",
    "feat_tsr = feat_tsr.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a41fed2",
   "metadata": {},
   "source": [
    "## 1.4 Done with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd3e3ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " labels.shape\n",
    "# size of training: 496; we only have labels for training set \n",
    "# y = torch.from_numpy(labels)\n",
    "# labels.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b84e00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "# test_size = len(splits['idx_test'])\n",
    "\n",
    "# I use 9, because we have 7 classes (0:6), hence, any value >= 7 is ok for value of other rows (i.e., test set) \n",
    "y = np.array([-1 for _ in range(feat.shape[0])])\n",
    "\n",
    "# set labels for trainning set \n",
    "y[idx_train] = labels\n",
    "\n",
    "# convert labels as tensor\n",
    "y_tsr = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4db4019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 1984)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits['idx_train']), len(splits['idx_test'])\n",
    "# train VS. test\n",
    "# splits['idx_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cdb2d1",
   "metadata": {},
   "source": [
    "## 1.5 Create Train and Validation Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d16bb79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 99, 397, 99)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set \n",
    "validation_split = 0.2 # 80% for validation\n",
    "train_size = len(labels)\n",
    "\n",
    "# I split 20% of the traning set for validation. \n",
    "mysplit = int(np.floor(validation_split*train_size))\n",
    "\n",
    "#np.random.shuffle(indices)\n",
    "\n",
    "# I set train_indices and val_indices\n",
    "train_indices, val_indices = idx_train[mysplit:], idx_train[:mysplit]\n",
    "\n",
    "\n",
    "# that is 397 for training and 99 for testing\n",
    "train_size, mysplit, len(train_indices), len(val_indices), \n",
    "#len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568991a",
   "metadata": {},
   "source": [
    "## 1.6 Create Tensor for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71c63dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a true or false tensor\n",
    "def getTensorBool(indices):\n",
    "    result = np.zeros(feat.shape[0], dtype=bool)\n",
    "    result[indices] = True\n",
    "    result = torch.from_numpy(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31c3523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tsr = getTensorBool(train_indices)\n",
    "val_tsr = getTensorBool(val_indices)\n",
    "#test_tsr = getTensorBool(idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af7205",
   "metadata": {},
   "source": [
    "## 1.7  Create Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69f95b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[2480, 1390], edge_index=[2, 10100], y=[2480], train_mask=[2480], val_mask=[2480], test_mask=False),\n",
       " torch.float32,\n",
       " torch.int64,\n",
       " torch.int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# create input\n",
    "# x: all the attributes for all nodes !!!\n",
    "# edge_index: graph structure !!!\n",
    "# y: all the labels \n",
    "# ...\n",
    "data = Data(x=feat_tsr, edge_index = edge_index, y = y_tsr, train_mask = train_tsr, val_mask=val_tsr, test_mask=False)#like a dictionary\n",
    "\n",
    "data, data.x.dtype, data.y.dtype, data.edge_index.dtype\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd2cd5",
   "metadata": {},
   "source": [
    "# 2 Set our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc43495",
   "metadata": {},
   "source": [
    "a model considering both network structure and features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cdaf97",
   "metadata": {},
   "source": [
    "## 2.1 Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "99076be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GraphConv, SAGEConv, APPNP\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # structure\n",
    "        self.conv1 = GCNConv(num_node_features, num_hidden) # two-layer GCN\n",
    "        self.conv2 = GCNConv(num_hidden, num_classes)\n",
    "        \n",
    "        # optional \n",
    "        #self.conv1 = APPNP(50, 0.1)\n",
    "        #self.conv2 = APPNP(20, 0.2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index # feature and structure\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x,training=self.training) # randomly dropout some features.\n",
    "        x = self.conv2(x, edge_index)\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)#calculate loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b4870",
   "metadata": {},
   "source": [
    "## 2.2 Optional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7c1f4b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.layer1 = nn.Linear(in_features, hidden_features) # nn.Linear is a class in PyTorch\n",
    "        self.layer2 = nn.Linear(hidden_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.view(-1, self.in_features)\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        return self.layer2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4607ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GraphConv, SAGEConv, APPNP\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # optional       \n",
    "        self.MLP = MLP(num_node_features, num_hidden, num_classes)\n",
    "        self.conv = APPNP(5,0.2) # k: 5, 10, 20, alpha: 0, 0.1, 0.2, 0.3\n",
    "        # k: 5; alpha: 0.2\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index # feature and structure\n",
    "        # appnp\n",
    "        x = self.MLP(x)\n",
    "        x = self.conv(x, edge_index) # appnp\n",
    "        return F.log_softmax(x, dim=1)#calculate loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf4823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "9de8608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN1(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # structure\n",
    "        self.conv1 = GraphConv(num_node_features, num_hidden) # two-layer GCN\n",
    "        self.conv2 = GraphConv(num_hidden, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index # feature and structure\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x,training=self.training) # randomly dropout some features.\n",
    "        x = self.conv2(x, edge_index)\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)#calculate loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "id": "4728d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN2(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # structure\n",
    "        self.conv1 = SAGEConv(num_node_features, num_hidden) # two-layer GCN\n",
    "        self.conv2 = SAGEConv(num_hidden, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index # feature and structure\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x,training=self.training) # randomly dropout some features.\n",
    "        x = self.conv2(x, edge_index)\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)#calculate loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7b1a0",
   "metadata": {},
   "source": [
    "## 2.3 Train Our Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a72f98",
   "metadata": {},
   "source": [
    "##### Hyperparameters we tuned: \n",
    "\n",
    "1) loss rate, 2) weight decay, and 3) num_hidden\n",
    "\n",
    "##### After tuning, we decide to choose:\n",
    "\n",
    "1) 0.01, 2) 0.001, and 3) 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67091e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1.9260088205337524 ACC: 0.2828282828282828\n",
      "Epoch 10: 0.4641801416873932 ACC: 0.797979797979798\n",
      "Epoch 20: 0.06438255310058594 ACC: 0.7878787878787878\n",
      "Epoch 30: 0.035822365432977676 ACC: 0.7878787878787878\n",
      "Epoch 40: 0.046079929918050766 ACC: 0.8181818181818182\n",
      "Epoch 50: 0.04308245703577995 ACC: 0.8282828282828283\n",
      "Epoch 60: 0.03629332780838013 ACC: 0.8181818181818182\n",
      "Epoch 70: 0.03363652899861336 ACC: 0.8181818181818182\n",
      "Epoch 80: 0.03139854967594147 ACC: 0.8181818181818182\n",
      "Epoch 90: 0.02937909960746765 ACC: 0.8080808080808081\n",
      "Epoch 100: 0.027875317260622978 ACC: 0.8080808080808081\n",
      "Epoch 110: 0.026622600853443146 ACC: 0.8080808080808081\n",
      "Epoch 120: 0.025604259222745895 ACC: 0.8080808080808081\n",
      "Epoch 130: 0.024743717163801193 ACC: 0.8080808080808081\n",
      "Epoch 140: 0.024038663133978844 ACC: 0.8080808080808081\n",
      "Epoch 150: 0.023449977859854698 ACC: 0.8080808080808081\n",
      "Epoch 160: 0.022945530712604523 ACC: 0.8080808080808081\n",
      "Epoch 170: 0.02251851186156273 ACC: 0.8080808080808081\n",
      "Epoch 180: 0.02215689979493618 ACC: 0.8080808080808081\n",
      "Epoch 190: 0.02185792289674282 ACC: 0.8080808080808081\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "\n",
    "# setting the model\n",
    "model = GCN(num_node_features=data.x.shape[1], \n",
    "            num_hidden=64, #46?\n",
    "            num_classes=(data.y.max()+1).item() #label\n",
    "           ).to(device)\n",
    "\n",
    "# set learning rate \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-3)\n",
    "# ...\n",
    "# training the model\n",
    "model.train() # train model\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad() #clear the gradient \n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        acc = (out.argmax(axis=1)[data.val_mask].eq(data.y[data.val_mask]).sum().item())/(data.val_mask.sum().item())\n",
    "        print('Epoch {0}: {1}'.format(epoch, loss.item()), 'ACC:', acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ec52e",
   "metadata": {},
   "source": [
    "### 2.3.1 Evaluate our Model Using Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95febcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7980\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# evaluate validation group\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "acc = int(correct) / int(data.val_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a136db0b",
   "metadata": {},
   "source": [
    "## 2.4 Prepare for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1a23e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pred[idx_test]\n",
    "np.savetxt('submission0.txt', preds, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a50d16b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 2, 6,  ..., 1, 1, 5])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e95c34a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "2\n",
      "6\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "6\n",
      "1\n",
      "0\n",
      "0\n",
      "6\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "6\n",
      "0\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "6\n",
      "1\n",
      "1\n",
      "3\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "6\n",
      "0\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "1\n",
      "6\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "4\n",
      "0\n",
      "3\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "6\n",
      "3\n",
      "6\n",
      "6\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "6\n",
      "2\n",
      "6\n",
      "4\n",
      "6\n",
      "1\n",
      "2\n",
      "6\n",
      "1\n",
      "5\n",
      "2\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "6\n",
      "0\n",
      "2\n",
      "6\n",
      "0\n",
      "5\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "6\n",
      "4\n",
      "2\n",
      "6\n",
      "2\n",
      "0\n",
      "5\n",
      "1\n",
      "0\n",
      "5\n",
      "3\n",
      "0\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "6\n",
      "6\n",
      "1\n",
      "2\n",
      "0\n",
      "4\n",
      "6\n",
      "1\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "3\n",
      "6\n",
      "6\n",
      "1\n",
      "6\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "1\n",
      "1\n",
      "5\n",
      "6\n",
      "3\n",
      "5\n",
      "0\n",
      "4\n",
      "4\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "6\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "4\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "5\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "6\n",
      "3\n",
      "2\n",
      "6\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "2\n",
      "1\n",
      "6\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "0\n",
      "5\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "6\n",
      "1\n",
      "0\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "6\n",
      "2\n",
      "6\n",
      "6\n",
      "3\n",
      "0\n",
      "2\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "5\n",
      "6\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "4\n",
      "3\n",
      "4\n",
      "1\n",
      "4\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "4\n",
      "2\n",
      "4\n",
      "6\n",
      "3\n",
      "2\n",
      "1\n",
      "5\n",
      "6\n",
      "0\n",
      "6\n",
      "2\n",
      "3\n",
      "5\n",
      "2\n",
      "6\n",
      "2\n",
      "6\n",
      "1\n",
      "3\n",
      "2\n",
      "6\n",
      "4\n",
      "6\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "5\n",
      "6\n",
      "3\n",
      "6\n",
      "2\n",
      "6\n",
      "2\n",
      "3\n",
      "2\n",
      "6\n",
      "6\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "1\n",
      "4\n",
      "6\n",
      "2\n",
      "3\n",
      "0\n",
      "2\n",
      "0\n",
      "5\n",
      "1\n",
      "2\n",
      "0\n",
      "6\n",
      "6\n",
      "1\n",
      "1\n",
      "6\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "6\n",
      "3\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "6\n",
      "1\n",
      "3\n",
      "6\n",
      "3\n",
      "6\n",
      "2\n",
      "6\n",
      "5\n",
      "2\n",
      "6\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "3\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "6\n",
      "0\n",
      "6\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "4\n",
      "2\n",
      "1\n",
      "6\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "0\n",
      "6\n",
      "4\n",
      "2\n",
      "4\n",
      "6\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "2\n",
      "6\n",
      "2\n",
      "1\n",
      "0\n",
      "5\n",
      "2\n",
      "4\n",
      "1\n",
      "6\n",
      "6\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "6\n",
      "0\n",
      "2\n",
      "6\n",
      "3\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "4\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "2\n",
      "3\n",
      "2\n",
      "6\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "5\n",
      "3\n",
      "2\n",
      "5\n",
      "3\n",
      "6\n",
      "6\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "4\n",
      "6\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "5\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "6\n",
      "2\n",
      "1\n",
      "6\n",
      "2\n",
      "3\n",
      "5\n",
      "2\n",
      "6\n",
      "3\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "0\n",
      "2\n",
      "6\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "0\n",
      "3\n",
      "6\n",
      "2\n",
      "6\n",
      "3\n",
      "5\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "6\n",
      "2\n",
      "3\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "2\n",
      "6\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "4\n",
      "3\n",
      "2\n",
      "5\n",
      "3\n",
      "1\n",
      "5\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "6\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "5\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "5\n",
      "2\n",
      "1\n",
      "4\n",
      "6\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "6\n",
      "6\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "5\n",
      "0\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "6\n",
      "2\n",
      "4\n",
      "6\n",
      "1\n",
      "6\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "5\n",
      "5\n",
      "0\n",
      "2\n",
      "6\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "6\n",
      "6\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "6\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "4\n",
      "1\n",
      "6\n",
      "2\n",
      "5\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "0\n",
      "6\n",
      "0\n",
      "4\n",
      "2\n",
      "1\n",
      "3\n",
      "5\n",
      "2\n",
      "0\n",
      "2\n",
      "6\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "0\n",
      "1\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "4\n",
      "2\n",
      "6\n",
      "6\n",
      "1\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "4\n",
      "3\n",
      "0\n",
      "6\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "2\n",
      "6\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "0\n",
      "2\n",
      "6\n",
      "4\n",
      "5\n",
      "2\n",
      "3\n",
      "3\n",
      "6\n",
      "0\n",
      "6\n",
      "6\n",
      "6\n",
      "2\n",
      "4\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "5\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "4\n",
      "2\n",
      "3\n",
      "0\n",
      "6\n",
      "3\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "6\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "0\n",
      "6\n",
      "6\n",
      "0\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "6\n",
      "6\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "6\n",
      "6\n",
      "5\n",
      "6\n",
      "1\n",
      "3\n",
      "5\n",
      "1\n",
      "6\n",
      "2\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "5\n",
      "3\n",
      "6\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "6\n",
      "5\n",
      "6\n",
      "3\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "2\n",
      "4\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "5\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "5\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "4\n",
      "3\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "6\n",
      "6\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "6\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "5\n",
      "3\n",
      "5\n",
      "1\n",
      "0\n",
      "1\n",
      "6\n",
      "2\n",
      "2\n",
      "5\n",
      "4\n",
      "6\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "5\n",
      "5\n",
      "6\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "4\n",
      "6\n",
      "3\n",
      "2\n",
      "6\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "3\n",
      "6\n",
      "0\n",
      "6\n",
      "1\n",
      "6\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "6\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "6\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "2\n",
      "4\n",
      "3\n",
      "0\n",
      "3\n",
      "6\n",
      "1\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "1\n",
      "4\n",
      "5\n",
      "2\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "4\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "6\n",
      "3\n",
      "2\n",
      "2\n",
      "5\n",
      "6\n",
      "1\n",
      "2\n",
      "1\n",
      "6\n",
      "2\n",
      "2\n",
      "4\n",
      "6\n",
      "0\n",
      "4\n",
      "3\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "6\n",
      "6\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "6\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "1\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "5\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "6\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "6\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "5\n",
      "4\n",
      "2\n",
      "0\n",
      "4\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "6\n",
      "3\n",
      "1\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "6\n",
      "1\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "2\n",
      "0\n",
      "3\n",
      "4\n",
      "2\n",
      "6\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "5\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "6\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "3\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "1\n",
      "6\n",
      "2\n",
      "1\n",
      "4\n",
      "4\n",
      "0\n",
      "2\n",
      "1\n",
      "4\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "6\n",
      "6\n",
      "3\n",
      "5\n",
      "2\n",
      "6\n",
      "0\n",
      "5\n",
      "3\n",
      "6\n",
      "0\n",
      "0\n",
      "4\n",
      "3\n",
      "1\n",
      "2\n",
      "4\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "6\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "4\n",
      "0\n",
      "6\n",
      "1\n",
      "4\n",
      "2\n",
      "6\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "6\n",
      "6\n",
      "6\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "2\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "4\n",
      "3\n",
      "3\n",
      "2\n",
      "4\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "4\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "6\n",
      "1\n",
      "2\n",
      "5\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "5\n",
      "1\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "6\n",
      "2\n",
      "6\n",
      "0\n",
      "2\n",
      "4\n",
      "3\n",
      "1\n",
      "3\n",
      "6\n",
      "6\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "6\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "6\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "0\n",
      "6\n",
      "4\n",
      "5\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "4\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "6\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "6\n",
      "4\n",
      "1\n",
      "2\n",
      "6\n",
      "6\n",
      "1\n",
      "3\n",
      "6\n",
      "2\n",
      "5\n",
      "6\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "4\n",
      "6\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "4\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "3\n",
      "6\n",
      "6\n",
      "1\n",
      "1\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "1\n",
      "6\n",
      "2\n",
      "3\n",
      "1\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "6\n",
      "2\n",
      "1\n",
      "3\n",
      "4\n",
      "6\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "6\n",
      "4\n",
      "1\n",
      "6\n",
      "0\n",
      "3\n",
      "1\n",
      "4\n",
      "0\n",
      "6\n",
      "2\n",
      "6\n",
      "2\n",
      "4\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "6\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "4\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "5\n",
      "5\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "4\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "5\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "0\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "5\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "6\n",
      "2\n",
      "6\n",
      "1\n",
      "4\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "4\n",
      "5\n",
      "4\n",
      "2\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "0\n",
      "6\n",
      "0\n",
      "6\n",
      "2\n",
      "1\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "2\n",
      "0\n",
      "6\n",
      "2\n",
      "1\n",
      "6\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "6\n",
      "6\n",
      "2\n",
      "6\n",
      "4\n",
      "5\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "6\n",
      "4\n",
      "3\n",
      "6\n",
      "2\n",
      "2\n",
      "5\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "6\n",
      "2\n",
      "2\n",
      "1\n",
      "5\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "5\n",
      "0\n",
      "1\n",
      "4\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "4\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "0\n",
      "4\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "6\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "5\n",
      "5\n",
      "4\n",
      "1\n",
      "2\n",
      "6\n",
      "0\n",
      "2\n",
      "6\n",
      "3\n",
      "6\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "2\n",
      "6\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "3\n",
      "5\n",
      "1\n",
      "2\n",
      "2\n",
      "6\n",
      "1\n",
      "4\n",
      "2\n",
      "6\n",
      "3\n",
      "6\n",
      "5\n",
      "1\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "5\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "0\n",
      "6\n",
      "5\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "6\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "6\n",
      "6\n",
      "2\n",
      "0\n",
      "1\n",
      "6\n",
      "1\n",
      "6\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "6\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "6\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "6\n",
      "4\n",
      "4\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "6\n",
      "3\n",
      "2\n",
      "5\n",
      "3\n",
      "2\n",
      "5\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in preds.numpy():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116cf8b",
   "metadata": {},
   "source": [
    "# 3 Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f4061",
   "metadata": {},
   "source": [
    "## 3.1 Emsemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26af7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 5\n",
    "models = []\n",
    "for i in range(num_models):\n",
    "    model = GCN(num_node_features=data.x.shape[1], \n",
    "            num_hidden=64, \n",
    "            num_classes=(data.y.max()+1).item() #label\n",
    "           ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-3)\n",
    "    model.train()\n",
    "    for epoch in range(101):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "676fe4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Accuracy: 0.9697\n",
      "Training Set Accuracy: 0.9521\n",
      "Validation Set Accuracy: 0.9697\n",
      "Training Set Accuracy: 0.9496\n",
      "Validation Set Accuracy: 0.9697\n",
      "Training Set Accuracy: 0.9547\n",
      "Validation Set Accuracy: 0.9697\n",
      "Training Set Accuracy: 0.9521\n",
      "Validation Set Accuracy: 0.9697\n",
      "Training Set Accuracy: 0.9496\n",
      "Validation Set Accuracy: 0.9697\n",
      "Training Set Accuracy: 0.9496\n",
      "Validation Set Accuracy: 0.9697\n",
      "Training Set Accuracy: 0.9496\n",
      "Validation Set Accuracy: 0.9697\n",
      "Training Set Accuracy: 0.9496\n",
      "Validation Set Accuracy: 0.9697\n",
      "Training Set Accuracy: 0.9496\n",
      "Validation Set Accuracy: 0.9697\n",
      "Training Set Accuracy: 0.9496\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    model.eval()\n",
    "    # evaluate validation group\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "    acc = int(correct) / int(data.val_mask.sum())\n",
    "    print(f'Validation Set Accuracy: {acc:.4f}')\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    correct = (pred[data.train_mask] == data.y[data.train_mask]).sum()\n",
    "    acc = int(correct) / int(data.train_mask.sum())\n",
    "    print(f'Training Set Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f7258707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 2, 4, 4, 6, 0, 6, 1, 3, 2, 2, 1, 3, 3, 0, 2, 3, 5, 0, 4, 1, 2, 5,\n",
      "        1, 5, 1, 1, 1, 2, 2, 3, 3, 3, 1, 3, 2, 2, 6, 2, 2, 2, 4, 2, 2, 6, 1, 2,\n",
      "        2, 2, 3, 4, 0, 2, 5, 2, 1, 6, 2, 2, 6, 2, 6, 4, 4, 4, 1, 3, 6, 0, 6, 3,\n",
      "        0, 0, 6, 6, 0, 2, 2, 3, 3, 6, 5, 1, 1, 1, 1, 2, 3, 3, 3, 0, 3, 1, 4, 2,\n",
      "        0, 1, 1])\n",
      "Accuracy: 0.9899\n",
      "tensor([1, 0, 2, 4, 4, 6, 0, 6, 1, 3, 2, 2, 1, 3, 3, 0, 2, 3, 5, 0, 4, 1, 2, 5,\n",
      "        1, 5, 1, 1, 1, 2, 2, 3, 3, 3, 1, 3, 2, 2, 6, 2, 2, 2, 4, 2, 2, 6, 1, 2,\n",
      "        2, 2, 3, 4, 0, 2, 5, 2, 1, 6, 2, 2, 6, 2, 6, 4, 4, 4, 1, 3, 6, 0, 6, 3,\n",
      "        0, 0, 6, 6, 0, 2, 2, 3, 3, 6, 5, 1, 1, 1, 1, 2, 3, 3, 3, 0, 3, 1, 4, 2,\n",
      "        0, 1, 1])\n",
      "Accuracy: 0.9899\n",
      "tensor([1, 0, 2, 4, 4, 6, 0, 6, 1, 3, 2, 2, 1, 3, 3, 0, 2, 3, 5, 0, 4, 1, 2, 5,\n",
      "        1, 5, 1, 1, 1, 2, 2, 3, 3, 3, 1, 3, 2, 2, 6, 2, 2, 2, 4, 2, 2, 6, 1, 2,\n",
      "        2, 2, 3, 4, 0, 2, 5, 2, 1, 6, 2, 2, 6, 2, 6, 4, 4, 4, 1, 3, 6, 0, 6, 3,\n",
      "        0, 0, 6, 6, 0, 2, 2, 3, 3, 6, 5, 1, 1, 1, 1, 2, 3, 3, 3, 0, 3, 1, 4, 2,\n",
      "        0, 1, 1])\n",
      "Accuracy: 0.9899\n",
      "tensor([1, 0, 2, 4, 4, 6, 0, 6, 1, 3, 2, 2, 1, 3, 3, 0, 2, 3, 5, 0, 4, 1, 2, 5,\n",
      "        1, 5, 1, 1, 1, 2, 2, 3, 3, 3, 1, 3, 2, 2, 6, 2, 2, 2, 4, 2, 2, 6, 1, 2,\n",
      "        2, 2, 3, 4, 0, 2, 5, 2, 1, 6, 2, 2, 6, 2, 6, 4, 4, 4, 1, 3, 6, 0, 6, 3,\n",
      "        0, 0, 6, 6, 0, 2, 2, 3, 3, 6, 5, 1, 1, 1, 1, 2, 3, 3, 3, 0, 3, 1, 4, 2,\n",
      "        0, 1, 1])\n",
      "Accuracy: 0.9899\n",
      "tensor([1, 0, 2, 4, 4, 6, 0, 6, 1, 3, 2, 2, 1, 3, 3, 0, 2, 3, 5, 0, 4, 1, 2, 5,\n",
      "        1, 5, 1, 1, 1, 2, 2, 3, 3, 3, 1, 3, 2, 2, 6, 2, 2, 2, 4, 2, 2, 6, 1, 2,\n",
      "        2, 2, 3, 4, 0, 2, 5, 2, 1, 6, 2, 2, 6, 2, 6, 4, 4, 4, 1, 3, 6, 0, 6, 3,\n",
      "        0, 0, 6, 6, 0, 2, 2, 3, 3, 6, 5, 1, 1, 1, 1, 2, 3, 3, 3, 0, 3, 1, 4, 2,\n",
      "        0, 1, 1])\n",
      "Accuracy: 0.9899\n",
      "tensor([1, 0, 2, 4, 4, 6, 0, 6, 1, 3, 2, 2, 1, 3, 3, 0, 2, 3, 5, 0, 4, 1, 2, 5,\n",
      "        1, 5, 1, 1, 1, 2, 2, 3, 3, 3, 1, 3, 2, 2, 6, 2, 2, 2, 4, 2, 2, 6, 1, 2,\n",
      "        2, 2, 3, 4, 0, 2, 5, 2, 1, 6, 2, 2, 6, 2, 6, 4, 4, 4, 1, 3, 6, 0, 6, 3,\n",
      "        0, 0, 6, 6, 0, 2, 2, 3, 3, 6, 5, 1, 1, 1, 1, 2, 3, 3, 3, 0, 3, 1, 4, 2,\n",
      "        0, 1, 1])\n",
      "Accuracy: 0.9899\n",
      "tensor([1, 0, 2, 4, 4, 6, 0, 6, 1, 3, 2, 2, 1, 3, 3, 0, 2, 3, 5, 0, 4, 1, 2, 5,\n",
      "        1, 5, 1, 1, 1, 2, 2, 3, 3, 3, 1, 3, 2, 2, 6, 2, 2, 2, 4, 2, 2, 6, 1, 2,\n",
      "        2, 2, 3, 4, 0, 2, 5, 2, 1, 6, 2, 2, 6, 2, 6, 4, 4, 4, 1, 3, 6, 0, 6, 3,\n",
      "        0, 0, 6, 6, 0, 2, 2, 3, 3, 6, 5, 1, 1, 1, 1, 2, 3, 3, 3, 0, 3, 1, 4, 2,\n",
      "        0, 1, 1])\n",
      "Accuracy: 0.9899\n",
      "tensor([1, 0, 2, 4, 4, 6, 0, 6, 1, 3, 2, 2, 1, 3, 3, 0, 2, 3, 5, 0, 4, 1, 2, 5,\n",
      "        1, 5, 1, 1, 1, 2, 2, 3, 3, 3, 1, 3, 2, 2, 6, 2, 2, 2, 4, 2, 2, 6, 1, 2,\n",
      "        2, 2, 3, 4, 0, 2, 5, 2, 1, 6, 2, 2, 6, 2, 6, 4, 4, 4, 1, 3, 6, 0, 6, 3,\n",
      "        0, 0, 6, 6, 0, 2, 2, 3, 3, 6, 5, 1, 1, 1, 1, 2, 3, 3, 3, 0, 3, 1, 4, 2,\n",
      "        0, 1, 1])\n",
      "Accuracy: 0.9899\n",
      "tensor([1, 0, 2, 4, 4, 6, 0, 6, 1, 3, 2, 2, 1, 3, 3, 0, 2, 3, 5, 0, 4, 1, 2, 5,\n",
      "        1, 5, 1, 1, 1, 2, 2, 3, 3, 3, 1, 3, 2, 2, 6, 2, 2, 2, 4, 2, 2, 6, 1, 2,\n",
      "        2, 2, 3, 4, 0, 2, 5, 2, 1, 6, 2, 2, 6, 2, 6, 4, 4, 4, 1, 3, 6, 0, 6, 3,\n",
      "        0, 0, 6, 6, 0, 2, 2, 3, 3, 6, 5, 1, 1, 1, 1, 2, 3, 3, 3, 0, 3, 1, 4, 2,\n",
      "        0, 1, 1])\n",
      "Accuracy: 0.9899\n",
      "tensor([1, 0, 2, 4, 4, 6, 0, 6, 1, 3, 2, 2, 1, 3, 3, 0, 2, 3, 5, 0, 4, 1, 2, 5,\n",
      "        1, 5, 1, 1, 1, 2, 2, 3, 3, 3, 1, 3, 2, 2, 6, 2, 2, 2, 4, 2, 2, 6, 1, 2,\n",
      "        2, 2, 3, 4, 0, 2, 5, 2, 1, 6, 2, 2, 6, 2, 6, 4, 4, 4, 1, 3, 6, 0, 6, 3,\n",
      "        0, 0, 6, 6, 0, 2, 2, 3, 3, 6, 5, 1, 1, 1, 1, 2, 3, 3, 3, 0, 3, 1, 4, 2,\n",
      "        0, 1, 1])\n",
      "Accuracy: 0.9899\n",
      "tensor([1, 0, 2, 4, 4, 6, 0, 6, 1, 3, 2, 2, 1, 3, 3, 0, 2, 3, 5, 0, 4, 1, 2, 5,\n",
      "        1, 5, 1, 1, 1, 2, 2, 3, 3, 3, 1, 3, 2, 2, 5, 2, 2, 2, 4, 2, 2, 6, 1, 2,\n",
      "        2, 2, 3, 4, 0, 2, 5, 2, 1, 6, 2, 2, 6, 2, 6, 4, 4, 4, 1, 3, 6, 0, 6, 3,\n",
      "        0, 0, 6, 6, 0, 2, 2, 3, 3, 6, 5, 1, 1, 1, 1, 2, 3, 3, 3, 0, 3, 1, 4, 2,\n",
      "        0, 1, 1])\n",
      "Accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "# evaluate per model\n",
    "model_predictions = []\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    # evaluate validation group\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    result = pred[data.val_mask]\n",
    "    print(result)\n",
    "    model_predictions.append(result.cpu().numpy())\n",
    "    #print((torch.from_numpy(result.numpy())==data.y[data.val_mask]).sum())\n",
    "    correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "    acc = int(correct) / int(data.val_mask.sum())\n",
    "    print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d3bf6821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qk/wp8q21b161jdcgqzqn5lwsch0000gn/T/ipykernel_84275/944257903.py:3: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  result = mode(np.stack(model_predictions), axis=0)[0][0]\n"
     ]
    }
   ],
   "source": [
    "# esemble the result\n",
    "from scipy.stats import mode\n",
    "result = mode(np.stack(model_predictions), axis=0)[0][0]\n",
    "acc = int((torch.from_numpy(result)==data.y[data.val_mask]).sum())/ int(data.val_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "319ed429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 4, 4, 6, 0, 6, 1, 3, 2, 2, 1, 3, 2, 0, 2, 3, 5, 0, 4, 1,\n",
       "       2, 5, 1, 5, 1, 1, 1, 2, 2, 3, 3, 3, 1, 3, 2, 2, 6, 2, 2, 2, 4, 2,\n",
       "       2, 6, 1, 2, 2, 2, 3, 4, 0, 2, 5, 2, 1, 6, 2, 2, 6, 2, 6, 4, 4, 4,\n",
       "       1, 3, 6, 0, 6, 3, 0, 0, 6, 6, 0, 2, 2, 3, 3, 6, 5, 1, 1, 1, 1, 2,\n",
       "       3, 3, 3, 0, 3, 1, 4, 2, 0, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb2754",
   "metadata": {},
   "source": [
    "## Result from Ensemble Method + Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6c6558fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 2, 6,  ..., 1, 1, 5])\n",
      "tensor([6, 2, 6,  ..., 1, 1, 5])\n",
      "tensor([6, 2, 6,  ..., 1, 1, 5])\n",
      "tensor([6, 2, 6,  ..., 1, 1, 5])\n",
      "tensor([6, 2, 6,  ..., 1, 1, 5])\n",
      "tensor([6, 2, 6,  ..., 1, 1, 5])\n",
      "tensor([6, 2, 6,  ..., 1, 1, 5])\n",
      "tensor([6, 2, 6,  ..., 1, 1, 5])\n",
      "tensor([6, 2, 6,  ..., 1, 1, 5])\n",
      "tensor([6, 2, 6,  ..., 1, 1, 5])\n",
      "tensor([6, 2, 6,  ..., 1, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "model_predictions = []\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    # evaluate validation group\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    result = pred[idx_test]\n",
    "    print(result)\n",
    "    model_predictions.append(result.cpu().numpy())\n",
    "    #print((torch.from_numpy(result.numpy())==data.y[data.val_mask]).sum())\n",
    "    #correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "    #acc = int(correct) / int(data.val_mask.sum())\n",
    "    #print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "10455772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qk/wp8q21b161jdcgqzqn5lwsch0000gn/T/ipykernel_84275/2193406015.py:1: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  result = mode(np.stack(model_predictions), axis=0)[0][0]\n"
     ]
    }
   ],
   "source": [
    "result = mode(np.stack(model_predictions), axis=0)[0][0]\n",
    "#(torch.from_numpy(result)==data.y[data.val_mask]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7f892db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "2\n",
      "6\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "6\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "6\n",
      "6\n",
      "0\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "6\n",
      "1\n",
      "1\n",
      "3\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "6\n",
      "0\n",
      "3\n",
      "0\n",
      "5\n",
      "5\n",
      "3\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "1\n",
      "6\n",
      "5\n",
      "5\n",
      "1\n",
      "6\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "4\n",
      "0\n",
      "3\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "6\n",
      "3\n",
      "6\n",
      "6\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "6\n",
      "2\n",
      "6\n",
      "1\n",
      "6\n",
      "1\n",
      "2\n",
      "6\n",
      "1\n",
      "5\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "2\n",
      "3\n",
      "0\n",
      "2\n",
      "6\n",
      "0\n",
      "5\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "3\n",
      "0\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "4\n",
      "6\n",
      "1\n",
      "2\n",
      "6\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "3\n",
      "6\n",
      "6\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "1\n",
      "1\n",
      "0\n",
      "6\n",
      "3\n",
      "6\n",
      "0\n",
      "4\n",
      "4\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "6\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "4\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "5\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "6\n",
      "3\n",
      "2\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "2\n",
      "1\n",
      "6\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "0\n",
      "5\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "6\n",
      "1\n",
      "0\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "6\n",
      "2\n",
      "6\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "3\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "5\n",
      "6\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "4\n",
      "3\n",
      "4\n",
      "1\n",
      "4\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "4\n",
      "2\n",
      "4\n",
      "6\n",
      "2\n",
      "2\n",
      "1\n",
      "5\n",
      "0\n",
      "0\n",
      "6\n",
      "2\n",
      "3\n",
      "5\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "6\n",
      "4\n",
      "6\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "5\n",
      "6\n",
      "3\n",
      "6\n",
      "2\n",
      "6\n",
      "2\n",
      "3\n",
      "2\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "2\n",
      "4\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "6\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "4\n",
      "1\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "6\n",
      "3\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "6\n",
      "1\n",
      "3\n",
      "6\n",
      "3\n",
      "6\n",
      "2\n",
      "6\n",
      "5\n",
      "2\n",
      "6\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "6\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "4\n",
      "2\n",
      "1\n",
      "6\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "0\n",
      "2\n",
      "4\n",
      "2\n",
      "4\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "5\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "4\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "6\n",
      "0\n",
      "2\n",
      "6\n",
      "3\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "4\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "5\n",
      "3\n",
      "2\n",
      "5\n",
      "3\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "6\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "6\n",
      "2\n",
      "3\n",
      "5\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "0\n",
      "2\n",
      "6\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "0\n",
      "3\n",
      "6\n",
      "2\n",
      "6\n",
      "3\n",
      "5\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "6\n",
      "2\n",
      "3\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "6\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "5\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "4\n",
      "3\n",
      "2\n",
      "5\n",
      "2\n",
      "1\n",
      "5\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "6\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "0\n",
      "5\n",
      "4\n",
      "6\n",
      "2\n",
      "2\n",
      "4\n",
      "0\n",
      "2\n",
      "1\n",
      "4\n",
      "6\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "4\n",
      "6\n",
      "6\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "5\n",
      "4\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "6\n",
      "3\n",
      "3\n",
      "6\n",
      "2\n",
      "4\n",
      "6\n",
      "1\n",
      "6\n",
      "2\n",
      "2\n",
      "4\n",
      "5\n",
      "5\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "6\n",
      "5\n",
      "0\n",
      "2\n",
      "6\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "6\n",
      "6\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "6\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "4\n",
      "1\n",
      "6\n",
      "2\n",
      "5\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "0\n",
      "6\n",
      "0\n",
      "6\n",
      "2\n",
      "6\n",
      "3\n",
      "5\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "4\n",
      "0\n",
      "4\n",
      "0\n",
      "1\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "1\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "4\n",
      "3\n",
      "0\n",
      "6\n",
      "3\n",
      "2\n",
      "3\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "2\n",
      "6\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "0\n",
      "2\n",
      "6\n",
      "4\n",
      "5\n",
      "2\n",
      "3\n",
      "3\n",
      "6\n",
      "0\n",
      "6\n",
      "6\n",
      "3\n",
      "2\n",
      "4\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "0\n",
      "6\n",
      "3\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "0\n",
      "6\n",
      "6\n",
      "0\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "6\n",
      "6\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "6\n",
      "6\n",
      "0\n",
      "6\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "6\n",
      "2\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "4\n",
      "6\n",
      "3\n",
      "6\n",
      "5\n",
      "3\n",
      "6\n",
      "2\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "5\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "6\n",
      "0\n",
      "6\n",
      "3\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "2\n",
      "4\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "5\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "5\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "4\n",
      "3\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "6\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "6\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "5\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "6\n",
      "4\n",
      "6\n",
      "0\n",
      "2\n",
      "4\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "5\n",
      "6\n",
      "4\n",
      "0\n",
      "6\n",
      "5\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "4\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "3\n",
      "6\n",
      "0\n",
      "6\n",
      "1\n",
      "6\n",
      "1\n",
      "6\n",
      "2\n",
      "1\n",
      "6\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "6\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "6\n",
      "1\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "1\n",
      "4\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "6\n",
      "3\n",
      "4\n",
      "0\n",
      "4\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "6\n",
      "3\n",
      "2\n",
      "2\n",
      "5\n",
      "6\n",
      "1\n",
      "2\n",
      "1\n",
      "6\n",
      "2\n",
      "2\n",
      "4\n",
      "6\n",
      "0\n",
      "4\n",
      "3\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "6\n",
      "6\n",
      "2\n",
      "0\n",
      "6\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "1\n",
      "4\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "5\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "6\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "6\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "6\n",
      "5\n",
      "4\n",
      "2\n",
      "0\n",
      "4\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "6\n",
      "3\n",
      "1\n",
      "2\n",
      "4\n",
      "1\n",
      "3\n",
      "6\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "6\n",
      "1\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "2\n",
      "0\n",
      "3\n",
      "4\n",
      "2\n",
      "6\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "6\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "3\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "1\n",
      "6\n",
      "2\n",
      "1\n",
      "4\n",
      "4\n",
      "0\n",
      "2\n",
      "1\n",
      "4\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "6\n",
      "6\n",
      "3\n",
      "5\n",
      "2\n",
      "6\n",
      "2\n",
      "5\n",
      "3\n",
      "6\n",
      "0\n",
      "2\n",
      "4\n",
      "3\n",
      "1\n",
      "2\n",
      "4\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "3\n",
      "6\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "4\n",
      "0\n",
      "6\n",
      "4\n",
      "4\n",
      "2\n",
      "6\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "6\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "5\n",
      "2\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "4\n",
      "2\n",
      "3\n",
      "2\n",
      "4\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "4\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "6\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "4\n",
      "0\n",
      "1\n",
      "6\n",
      "2\n",
      "6\n",
      "0\n",
      "2\n",
      "4\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "6\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "6\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "6\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "6\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "0\n",
      "6\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "5\n",
      "6\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "4\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "4\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "6\n",
      "2\n",
      "3\n",
      "6\n",
      "6\n",
      "1\n",
      "1\n",
      "0\n",
      "6\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "6\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "6\n",
      "4\n",
      "1\n",
      "0\n",
      "4\n",
      "6\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "6\n",
      "4\n",
      "1\n",
      "6\n",
      "0\n",
      "3\n",
      "1\n",
      "4\n",
      "0\n",
      "0\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "0\n",
      "4\n",
      "2\n",
      "2\n",
      "6\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "6\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "5\n",
      "5\n",
      "6\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "0\n",
      "6\n",
      "0\n",
      "4\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "5\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "0\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "5\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "6\n",
      "2\n",
      "6\n",
      "1\n",
      "4\n",
      "3\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "4\n",
      "5\n",
      "4\n",
      "2\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "0\n",
      "1\n",
      "0\n",
      "6\n",
      "2\n",
      "1\n",
      "0\n",
      "4\n",
      "3\n",
      "3\n",
      "2\n",
      "0\n",
      "6\n",
      "2\n",
      "1\n",
      "6\n",
      "0\n",
      "3\n",
      "0\n",
      "4\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "6\n",
      "6\n",
      "2\n",
      "0\n",
      "4\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "6\n",
      "4\n",
      "3\n",
      "6\n",
      "2\n",
      "2\n",
      "5\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "5\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "1\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "0\n",
      "5\n",
      "0\n",
      "4\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "6\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "0\n",
      "5\n",
      "4\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "4\n",
      "2\n",
      "6\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "6\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "2\n",
      "6\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "6\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "5\n",
      "6\n",
      "2\n",
      "0\n",
      "1\n",
      "6\n",
      "1\n",
      "6\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "6\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "6\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "6\n",
      "4\n",
      "4\n",
      "2\n",
      "6\n",
      "6\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "6\n",
      "3\n",
      "2\n",
      "5\n",
      "3\n",
      "2\n",
      "5\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in result:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a63ccec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result\n",
    "np.savetxt('submission999.txt', result, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b570e",
   "metadata": {},
   "source": [
    "## 3.2 Cross Validation + Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "157bf21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a087f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_cv(lr, wdecay, num_hidden, data, train_indices_):\n",
    "    avg = 0\n",
    "    num_models = 5\n",
    "    models = []\n",
    "    for fold, (train_indices, val_indices) in enumerate(kfold.split(train_indices_)):  \n",
    "    # setting the model\n",
    "        model = GCN(num_node_features=data.x.shape[1], \n",
    "            num_hidden=num_hidden, #46?\n",
    "            num_classes=7 #label\n",
    "           ).to(device)\n",
    "        data_k = Data(x=feat_tsr, \n",
    "                edge_index = edge_index, \n",
    "                y = y_tsr, \n",
    "                train_mask = getTensorBool(np.array(splits['idx_train'])[train_indices]), \n",
    "                val_mask = getTensorBool(np.array(splits['idx_train'])[val_indices]), test_mask=False)#like a dictionary\n",
    "        device_k = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        data_k = data_k.to(device_k)\n",
    "        # set learning rate \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "        for epoch in range(50):\n",
    "            optimizer.zero_grad() #clear the gradient \n",
    "            out = model(data_k)\n",
    "            loss = F.nll_loss(out[data_k.train_mask], data.y[data_k.train_mask])\n",
    "            loss.backward() #calculate the gradients\n",
    "            optimizer.step() # the step() is to update the param\n",
    "    \n",
    "            # evaluate validation group\n",
    "            model.eval()\n",
    "            pred = model(data).argmax(dim=1)\n",
    "            correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "            acc = int(correct) / int(data.val_mask.sum())\n",
    "            avg+=acc\n",
    "            if epoch % 10 == 0:\n",
    "                print('Epoch {0}: {1}'.format(epoch, loss.item()))\n",
    "                print(f'Accuracy per 10 Epoch: {acc:.4f}')\n",
    "        #print(model)\n",
    "        models.append(model)\n",
    "                \n",
    "                \n",
    "            \n",
    "    avg = avg/250\n",
    "    print(f'Accuracy: {avg:.4f}', lr, wdecay, num_hidden)\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a588052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = k_cv(0.01, 0.001, 128, data, train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in [1e-2, 1e-3]:\n",
    "    for wd in [1e-3, 1e-4, 5e-4]:\n",
    "        for num_hidden in [16,32,64,128] :\n",
    "            k_cv(lr, wd, num_hidden,data, train_indices)\n",
    "#Accuracy: 0.8804 0.01 0.001 16\n",
    "#Accuracy: 0.9102 0.01 0.001 32\n",
    "#Accuracy: 0.9273 0.01 0.001 64             - good!\n",
    "#Accuracy: 0.9293 0.01 0.001 128            - good!\n",
    "#Accuracy: 0.8890 0.01 0.0001 16\n",
    "#Accuracy: 0.9121 0.01 0.0001 32             \n",
    "#Accuracy: 0.9257 0.01 0.0001 64             \n",
    "#Accuracy: 0.9346 0.01 0.0001 128            - this is good\n",
    "#Accuracy: 0.8768 0.01 0.0005 16\n",
    "#Accuracy: 0.9114 0.01 0.0005 32\n",
    "#Accuracy: 0.9259 0.01 0.0005 64\n",
    "#Accuracy: 0.9304 0.01 0.0005 128\n",
    "#Accuracy: 0.6095 0.001 0.001 16\n",
    "#Accuracy: 0.6705 0.001 0.001 32\n",
    "#Accuracy: 0.7539 0.001 0.001 64\n",
    "#Accuracy: 0.7971 0.001 0.001 128\n",
    "#Accuracy: 0.5186 0.001 0.0001 16\n",
    "#Accuracy: 0.7233 0.001 0.0001 32\n",
    "#Accuracy: 0.7406 0.001 0.0001 64\n",
    "#Accuracy: 0.7904 0.001 0.0001 128\n",
    "#Accuracy: 0.5942 0.001 0.0005 16\n",
    "#Accuracy: 0.7038 0.001 0.0005 32\n",
    "#Accuracy: 0.7662 0.001 0.0005 64\n",
    "#Accuracy: 0.8017 0.001 0.0005 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd21ce",
   "metadata": {},
   "source": [
    "## Cross validation + Ensemble  (Version 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ab735adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_cv(lr, wdecay, num_hidden, data, train_indices_):\n",
    "    avg = 0\n",
    "    num_models = 5\n",
    "    model = GCN(num_node_features=data.x.shape[1], \n",
    "            num_hidden=num_hidden, #46?\n",
    "            num_classes=7 #label\n",
    "           ).to(device)\n",
    "    max_acc = 0\n",
    "    model_opt = None\n",
    "    for fold, (train_indices, val_indices) in enumerate(kfold.split(train_indices_)):  \n",
    "    # setting the model\n",
    "        data_k = Data(x=feat_tsr, \n",
    "                edge_index = edge_index, \n",
    "                y = y_tsr, \n",
    "                train_mask = getTensorBool(np.array(splits['idx_train'])[train_indices]), \n",
    "                val_mask = getTensorBool(np.array(splits['idx_train'])[val_indices]), test_mask=False)#like a dictionary\n",
    "        device_k = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        data_k = data_k.to(device_k)\n",
    "        # set learning rate \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "        for epoch in range(50):\n",
    "            optimizer.zero_grad() #clear the gradient \n",
    "            out = model(data_k)\n",
    "            loss = F.nll_loss(out[data_k.train_mask], data.y[data_k.train_mask])\n",
    "            loss.backward() #calculate the gradients\n",
    "            optimizer.step() # the step() is to update the param\n",
    "    \n",
    "            # evaluate validation group\n",
    "            model.eval()\n",
    "            pred = model(data).argmax(dim=1)\n",
    "            correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "            acc = int(correct) / int(data.val_mask.sum())\n",
    "            avg+=acc\n",
    "            if(acc>max_acc): \n",
    "                model_opt = model\n",
    "                max_acc = acc\n",
    "            #if epoch % 10 == 0:\n",
    "                #print('Epoch {0}: {1}'.format(epoch, loss.item()))\n",
    "                #print(f'Accuracy per 10 Epoch: {acc:.4f}')\n",
    "        #print(model)\n",
    "        #models.append(model)\n",
    "                \n",
    "                \n",
    "            \n",
    "    avg = avg/250\n",
    "    print(f'Accuracy: {avg:.4f}', lr, wdecay, num_hidden)\n",
    "    return model_opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "04456662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9581 0.01 0.001 128\n",
      "GCN(\n",
      "  (MLP): MLP(\n",
      "    (layer1): Linear(in_features=1390, out_features=128, bias=True)\n",
      "    (layer2): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      "  (conv): APPNP(K=5, alpha=0.2)\n",
      ")\n",
      "Accuracy: 0.9604 0.01 0.001 128\n",
      "GCN(\n",
      "  (MLP): MLP(\n",
      "    (layer1): Linear(in_features=1390, out_features=128, bias=True)\n",
      "    (layer2): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      "  (conv): APPNP(K=5, alpha=0.2)\n",
      ")\n",
      "Accuracy: 0.9602 0.01 0.001 128\n",
      "GCN(\n",
      "  (MLP): MLP(\n",
      "    (layer1): Linear(in_features=1390, out_features=128, bias=True)\n",
      "    (layer2): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      "  (conv): APPNP(K=5, alpha=0.2)\n",
      ")\n",
      "Accuracy: 0.9594 0.01 0.001 128\n",
      "GCN(\n",
      "  (MLP): MLP(\n",
      "    (layer1): Linear(in_features=1390, out_features=128, bias=True)\n",
      "    (layer2): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      "  (conv): APPNP(K=5, alpha=0.2)\n",
      ")\n",
      "Accuracy: 0.9592 0.01 0.001 128\n",
      "GCN(\n",
      "  (MLP): MLP(\n",
      "    (layer1): Linear(in_features=1390, out_features=128, bias=True)\n",
      "    (layer2): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      "  (conv): APPNP(K=5, alpha=0.2)\n",
      ")\n",
      "Accuracy: 0.9588 0.01 0.001 128\n",
      "GCN(\n",
      "  (MLP): MLP(\n",
      "    (layer1): Linear(in_features=1390, out_features=128, bias=True)\n",
      "    (layer2): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      "  (conv): APPNP(K=5, alpha=0.2)\n",
      ")\n",
      "Accuracy: 0.9580 0.01 0.001 128\n",
      "GCN(\n",
      "  (MLP): MLP(\n",
      "    (layer1): Linear(in_features=1390, out_features=128, bias=True)\n",
      "    (layer2): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      "  (conv): APPNP(K=5, alpha=0.2)\n",
      ")\n",
      "Accuracy: 0.9592 0.01 0.001 128\n",
      "GCN(\n",
      "  (MLP): MLP(\n",
      "    (layer1): Linear(in_features=1390, out_features=128, bias=True)\n",
      "    (layer2): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      "  (conv): APPNP(K=5, alpha=0.2)\n",
      ")\n",
      "Accuracy: 0.9601 0.01 0.001 128\n",
      "GCN(\n",
      "  (MLP): MLP(\n",
      "    (layer1): Linear(in_features=1390, out_features=128, bias=True)\n",
      "    (layer2): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      "  (conv): APPNP(K=5, alpha=0.2)\n",
      ")\n",
      "Accuracy: 0.9591 0.01 0.001 128\n",
      "GCN(\n",
      "  (MLP): MLP(\n",
      "    (layer1): Linear(in_features=1390, out_features=128, bias=True)\n",
      "    (layer2): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      "  (conv): APPNP(K=5, alpha=0.2)\n",
      ")\n",
      "Accuracy: 0.9604 0.01 0.001 128\n",
      "GCN(\n",
      "  (MLP): MLP(\n",
      "    (layer1): Linear(in_features=1390, out_features=128, bias=True)\n",
      "    (layer2): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      "  (conv): APPNP(K=5, alpha=0.2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(1,12):\n",
    "    model = k_cv(0.01, 0.001, 128, data, train_indices)\n",
    "    print(model)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db108e0",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "b489f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(num_node_features=data.x.shape[1], \n",
    "            num_hidden=16, #46?\n",
    "            num_classes=(data.y.max()+1).item() #label\n",
    "           ).to(device)\n",
    "def train(model, data, num_epochs=30, learning_rate=1e-2, weight_decay=1e-5):\n",
    "    # define an optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=learning_rate, \n",
    "                                 weight_decay=weight_decay) # weight_decay is the L2 Regularization\n",
    "    \n",
    "    # define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad() #clear the gradient \n",
    "            \n",
    "        out = model(data) #foward\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward() #calculate the gradients\n",
    "            \n",
    "        optimizer.step() # the step() is to update the param\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch {0}: {1}'.format(epoch, loss.item()))\n",
    "            model.eval()\n",
    "            # evaluate validation group\n",
    "            pred = model(data).argmax(dim=1)\n",
    "            correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "            acc = int(correct) / int(data.val_mask.sum())\n",
    "            print(f'Accuracy: {acc:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e58f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nepoch in range(4,10):\n",
    "    print(nepoch)\n",
    "    train(model, data_, 2**nepoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "1763efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2.3089566230773926\n",
      "Accuracy: 0.3145\n",
      "Epoch 10: 1.1150527000427246\n",
      "Accuracy: 0.7903\n",
      "Epoch 20: 0.42526018619537354\n",
      "Accuracy: 0.8548\n"
     ]
    }
   ],
   "source": [
    "train(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e8b20",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "id": "e6e84e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8687\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# evaluate validation group\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()\n",
    "acc = int(correct) / int(data.val_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1254,
   "id": "a3a816e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# evaluate training group\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.train_mask] == data.y[data.train_mask]).sum()\n",
    "acc = int(correct) / int(data.train_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "id": "b6e14a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2015\n"
     ]
    }
   ],
   "source": [
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data_.train_mask] == data_.y[data_.train_mask]).sum()\n",
    "acc = int(correct) / int(data_.train_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "id": "1fd3762d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data_).argmax(dim=1)\n",
    "correct = (pred[data_.test_mask] == data_.y[data_.test_mask]).sum()\n",
    "acc = int(correct) / int(data_.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20c236e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 2, 0,  ..., 1, 2, 6])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[data.test_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b828c7",
   "metadata": {},
   "source": [
    "# check my new result with old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "92c34224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 2, 6,  ..., 1, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "preds = pred[idx_test]\n",
    "print(preds)\n",
    "data.test_mask.shape\n",
    "np.savetxt('submission.txt', preds, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "id": "428d40d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 2, 0,  ..., 1, 2, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qk/wp8q21b161jdcgqzqn5lwsch0000gn/T/ipykernel_9712/2387635962.py:1: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  result = mode(np.stack(model_predictions), axis=0)[0][0]\n"
     ]
    }
   ],
   "source": [
    "result = mode(np.stack(model_predictions), axis=0)[0][0]\n",
    "result = torch.from_numpy(result)\n",
    "print(result)\n",
    "np.savetxt('submission5.txt', result, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4dcfb4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = np.loadtxt('/Users/xinyun/Desktop/2023Spring/CSE881/submission.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "966ea7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 2., 6., ..., 1., 1., 5.])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "id": "09d48455",
   "metadata": {},
   "outputs": [],
   "source": [
    "last = np.loadtxt('/Users/xinyun/Desktop/2023Spring/CSE881/submission7.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "id": "58197caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_ = np.loadtxt('/Users/xinyun/Desktop/2023Spring/CSE881/submission2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3c9b8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "old = np.loadtxt('/Users/xinyun/Downloads/submission.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "fede7cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 2., 0., ..., 1., 2., 6.])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d1c9c7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17893145161290322"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(today==old)/old.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "id": "2ba4b7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(last==today)/old.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "7f831968",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_optimize = {\n",
    "    'num_hidden': [32, 64, 128],\n",
    "    'lr': [0.001, 0.01, 0.1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "2acd3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "grid_search = GridSearchCV(model, params_to_optimize, cv=kfold, scoring='accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
